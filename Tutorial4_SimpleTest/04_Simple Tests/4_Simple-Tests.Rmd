---
title: "Simple Tests"
output:

#  pdf_document:
#    toc: true

  rmdformats::material:
    css: boxes.css

#  prettydoc::html_pretty:
#    theme: cayman
#    toc: true
#    toc_depth: 2
#    css: boxes.css


# Necessary only for pdf:

header-includes:
    - \usepackage[most]{tcolorbox}
    - \definecolor{light-yellow}{rgb}{1, 0.95, 0.7}
    - \newtcolorbox{myquote}{colback=light-yellow,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
    - \newcommand{\yellowbox}[1]{\begin{myquote} \textbf{#1} \end{myquote}}
    
    - \definecolor{light-blue}{rgb}{0.80,0.97,1.00}
    - \newtcolorbox{myquote2}{colback=light-blue,grow to right by=-2mm,grow to left by=-2mm, boxrule=0pt,boxsep=0pt,breakable}
    - \newcommand{\bluebox}[1]{\begin{myquote2} \text{#1} \end{myquote2}}

    - \definecolor{greenwhite}{rgb}{0.90,1.00,0.93}
    - \newtcolorbox{myquote3}{colback=greenwhite,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
    - \newcommand{\greenwhitebox}[1]{\begin{myquote3} \textbf{#1} \end{myquote3}}
---

```{r global_options, include = FALSE, eval = T}
knitr::opts_chunk$set(echo = T, # include code 
                      warning = F, # print warnings
                      message = F, # print messages
                      eval = F, # evaluate code
                      fig.width = 4.5,
                      fig.height = 3.5)

root_dir <- "C:/Users/fried/Dropbox/00_Work/2021-2022_HVW_Lab/NOW/SoftwareForAnalysingData_2021/Data"

knitr::opts_knit$set(root.dir = root_dir) 

knitr::opts_chunk$set(root.dir = root_dir) 
```

--- 


```{r, eval = T, include = F}
distr <- paste(root_dir,"/Distributions.csv", sep = "")
distr <- read.csv(distr)
choles <- paste(root_dir,"/Cholesterol.csv", sep = "")
choles <- read.csv(choles)
crime <- paste(root_dir,"/Crime.csv", sep = "")
crime <- read.csv(crime)
```

# Introduction

## Preface

In this document, we will go through some of the most common simple statistical tests. For each of them the corresponding question of interest, statistical hypotheses and decision criterion will be named will be named. Afterwards, descriptive statistics, the statistical test and a very basic visualization will be done on an example data set.
The following data sets will be used for the examples shown in this document. You can find the data in the folder "Data".

```{r, eval = F}
distr <- read.csv("Distributions.csv")
choles <- read.csv("Cholesterol.csv")
crime <- read.csv("Crime.csv")
```

`Distributions.csv` is a data set consisting of artificial data (two variables).

`Cholesterol.csv` contains longitudinal data on 18 people consuming margarine in order to reduce blood cholesterol levels.

`Crime.csv` includes of a variety of variables by US state.

\newpage

## General idea

Imagine you receive a data set and someone tells you "Please analyse it for me". 
What would you do? One way is to follow a hypotheses driven approach using frequentist statistics. The roadmap for this approach will be covered in this document. 
Starting with a research question, these are the steps you should follow:

1. __Formulate research question__ (research hypotheses) <br>    

This question is "just a regular question", for example "Does medication A work better than medication B in treating a certain disease". It can help to make it more specific, e.g. specifying what "better" means in this context. It is e.g. a good idea to include the independent (IV) and dependent variable (DV). In this example the IV would be "type of medication" and the DV "fever in °C". Knowing your DV and IV can also help you choose an appropriate statistical test.

2. __Specify statistical hypotheses__ <br>  

In this approach, they *always* come in pairs. The H0 usually states that there is no effect (e.g. no difference between medication A and B) and H1. They need to be *mutually exclusive* and "all" scenarios need to be covered.

\greenwhitebox{H0: "no differences", "no effect" (includes =, $\leq$ or $\geq$) \newline
           H1: "differences", "significant effect" (includes $\neq$, < or >)}

:::: {.greenbox data-latex=""}
::: {.center data-latex=""}
H0: "no differences", "no effect" (includes =, $\leq$ or $\geq$)
:::
::: {.center data-latex=""}
H1: "differences", "significant effect" (includes $\neq$, < or >)
:::
::::

3. __Set criterion__ 

In this case, it is the $\alpha$-niveau with which the p-value will be compared.
If p-value < $\alpha$ ("significance level"), we reject H0 in favor of H1. 

\yellowbox{P-Value: Probability of observing the data given H0}

:::: {.yellowbox data-latex=""}
::: {.center data-latex=""}
__P-Value__ \newline
:::
::: {.center data-latex=""}
Probability of observing the data given H0
:::
::::  

```{r, eval = F, include = F, }
:::: {.yellowbox data-latex=""}
::: {.center data-latex=""}
__P-Value__ \newline
:::
::: {.center data-latex=""}
Probability of observing the data given H0
:::
::::   
```

4. __Perform test__ 

Now we perform the chosen statistical test and interpret the output to answer the research question. Again: If the p-value is very small, we reject H0 in favor of H1 and assume that H1 is valid.

It can also be helpful to use descriptive statistics and to visualize the data before conducting a test in order to "get to know" the data. Also, they are crucial for communicating the results later.

## Assumptions for parametric tests

There are some assumptions for parametric tests:

- *Normality*: data are normally distributed
- *Homogeneity of Variances*: data from multiple groups have the same variance
- *Independence*: data are independent

_Note: Here, assumptions will not be explicitly tested in order to keep the focus on the tests themselves for now. In practice, the respective prerequisites should be checked before conducting a test._ 

\newpage

# Shapiro-Wilk test

This is a test to check for normally distributed data. As normally distributed data is a prerequisite for many tests, this test is widely used. (Yet, there are other tests and ways to achieve the same.) 

\bluebox{Question: Is the data normally distributed?}

:::: {.bluebox data-latex=""}
::: {.center2 data-latex=""}
Question: Is the data normally distributed?
:::
::::  
 

### __--- Hypotheses ---__

\greenwhitebox{H0: not significantly different to normal distribution \newline 
H1: significantly different to normal distribution}

:::: {.greenbox data-latex=""}
::: {.center data-latex=""}
H0: not significantly different to normal distribution 
:::
::: {.center data-latex=""}
H1: significantly different to normal distribution
:::
::::  

\yellowbox{p > 0.05 → normally distributed \newline
p $\leq$ 0.05 → not normally distributed}

:::: {.yellowbox data-latex=""}
::: {.center data-latex=""}
p > 0.05 → normally distributed 
:::
::: {.center data-latex=""}
p $\leq$ 0.05 → not normally distributed
:::
::::  

### __--- Descriptives ---__

```{r, eval = T}
summary(distr) 
```

Descriptive statistics may already give a hint: If mean and median differ a lot, this points towards the data most likely not being normally distributed.

### __--- Visualization ---__

Visual inspection is a (heuristic) way to check for normal distribution: The better a histogram of the data approaches a nice bell-shape, the more likely it is to be normally distributed.

```{r, eval = T, fig.width = 5, fig.height= 1.7}
par(mar = c(1,1,1,1), mfrow=c(1,2)) # ignore this line for now
hist(distr$ex1); hist(distr$ex2)
```

I'd say `ex1` could follow a normal distribution, `ex2` looks rather skewed.

### __--- Perform the test ---__

```{r, eval = T}
shapiro.test(distr$ex1)
shapiro.test(distr$ex2)
```


### __--- Answer ---__

Ex1 is normally distributed, Ex2 is not normally distributed.


\newpage
# F-test

Like the Shapiro-Wilk, this test is often used to check an assumption of some tests: Equal variances.

*Note: If you want to compare variances of more than two groups, you should use a different test. This will be covered later.* 

\bluebox{Question: Are variances in two groups equal?}

:::: {.bluebox data-latex=""}
::: {.center2 data-latex=""}
Question: Are variances in two groups equal?
:::
:::: 

### __--- Hypotheses ---__

\greenwhitebox{H0: Variances are equal \newline 
H1: Variances are not equal}

:::: {.greenbox data-latex=""}
::: {.center data-latex=""}
H0: Variances are equal
:::
::: {.center data-latex=""}
H1: Variances are not equal
:::
::::

\yellowbox{p > 0.05 → variances are equal \newline
p $\leq$ 0.05 → variances are not equal}

:::: {.yellowbox data-latex=""}
::: {.center data-latex=""}
p > 0.05 → variances are equal
:::
::: {.center data-latex=""}
p $\leq$ 0.05 → variances are not equal
:::
::::  

### __--- Visualization ---__

```{r, eval = T, fig.height= 3, fig.width= 3}
boxplot(distr)
```

The heights of the boxes look pretty similar. This points towards equal variances.

### __--- Perform the test ---__

```{r, eval = T}
var.test(distr$ex1, distr$ex2)
```

### __--- Answer ---__

The variance in the first group is equal to the variance in the second group.

\newpage

# T-tests

T-tests are used to check for significant differences in means between two groups. 

*Note: If there are more than two groups, you should either use an ANOVA or adjust the p-values. More on that will be covered in a later session*

## a) Two-sided

\bluebox{Question: Do mean scores of a certain variable differ significantly between two groups?}

:::: {.bluebox data-latex=""}
::: {.center2 data-latex=""}
Question: Do mean scores of a certain variable differ significantly between two groups?
:::
:::: 

*Example: Does the cholesterol level after 8 weeks differ between the groups?*

### __--- Hypotheses ---__

\greenwhitebox{H0: mean(groupA) = mean(groupB) \newline
H1: mean(groupA) $\neq$ mean(groupB)}


:::: {.greenbox data-latex=""}
::: {.center data-latex=""}
H0: mean(groupA) = mean(groupB)
:::
::: {.center data-latex=""}
H1: mean(groupA) $\neq$ mean(groupB)
:::
::::

\yellowbox{p > 0.05 → means are equal \newline
p $\leq$ 0.05 → means are not equal}


:::: {.yellowbox data-latex=""}
::: {.center data-latex=""}
p > 0.05 → means are equal
:::
::: {.center data-latex=""}
p $\leq$ 0.05 → means are not equal
:::
::::  
 

### __--- Descriptives ---__ 

```{r, eval = T}
mean(choles$After8weeks[choles$Margarine=="A"])
mean(choles$After8weeks[choles$Margarine=="B"])
```

The means differ... but is the difference significant?

### __--- Visualization ---__

```{r, eval= T, fig.height=3, fig.width= 3, fig.margin = F}
par(mar = c(2,2,1,1)) # ignore this line 
boxplot(choles$After8weeks[choles$Margarine=="A"],
        choles$After8weeks[choles$Margarine=="B"])
```

Boxes that do not "overlap" (here they do) would be a sign for a significant difference between the groups.

### __--- Perform the test ---__

```{r, eval = T}
t.test(choles$After8weeks[choles$Margarine=="A"],
       choles$After8weeks[choles$Margarine=="B"],
       paired = F,                # independent
       alternative = "two.sided") # two-sided)
```

### __--- Answer ---__

The cholesterol levels do not differ significantly between the two groups (types of margarine).



## b) One-sided

\bluebox{Question: Is the mean in one group significantly smaller (larger) than the mean of another group?}

:::: {.bluebox data-latex=""}
::: {.center2 data-latex=""}
Question: Is the mean in one group significantly smaller (larger) than the mean of another group?
:::
:::: 

*Example: Is the cholesterol level significantly lower after the intervention?*


### __--- Hypotheses ---__  

\greenwhitebox{H0: mean(groupA) $\geq$ mean(groupB) \newline
H1: mean(groupA) < mean(groupB)}


:::: {.greenbox data-latex=""}
::: {.center data-latex=""}
H0: mean(groupA) $\geq$ mean(groupB)
:::
::: {.center data-latex=""}
H1: mean(groupA) < mean(groupB)
:::
::::  
 

\yellowbox{p > 0.05 → mean(groupA) is $\geq$ to mean(groupB) \newline
p $\leq$ 0.05 → mean(groupA) is < than mean(groupB)}

:::: {.yellowbox data-latex=""}
::: {.center data-latex=""}
p > 0.05 → mean(groupA) is $\geq$ to mean(groupB)
:::
::: {.center data-latex=""}
p $\leq$ 0.05 → mean(groupA) is < than mean(groupB)
:::
::::  
 
*You could swap the order of the groups or formulate the hypotheses using $\leq$ instead of $\geq$ and > instead of <.*

### __--- Descriptives ---__

```{r, eval = T}
mean(choles$Before)
mean(choles$After8weeks)
```

Again, different means - but significantly different?

### __--- Visualization ---__

```{r, eval = T, fig.height= 3, fig.width= 3}
par(mar = c(2,2,1,1)) # ignore this line
boxplot(choles$Before, choles$After8weeks)
```

Again, the boxes overlap... Let's see what the statistical test says. 

### __--- Perform the test ---__

Here, we are using `paired = T` because we have a "within-subject design" where a variable is measured "within" a person over multiple points in time. `alternative` is set to `less` in order to specify that we have a one-sided hypothesis.

```{r, eval = T}
t.test(choles$After8weeks, 
       choles$Before,
       paired = T,              # dependent, because of "within-subject design"
       alternative = "less")    # one-sided
```

### __--- Answer ---__

The cholesterol level is significantly lower after 8 weeks.

\newpage

__A word on syntax...__

In R, there are usually many different ways how to achieve the same thing. Often, the choice of which one to use just depends on personal taste. Here an example.

```{r}
# Version 1: traditional interface
t.test(ds$dV[ds$group==1], # ds = data set,
       ds$dV[ds$group==2]) # dV = dependent Variable

# Version 2: variation of traditional interface
with(ds, t.test(dV[group == 1], 
                dV[group == 2]))

# Version 3: formula interface
t.test(dV ~ group, data = ds)
```

Which is one is your favorite? Write it into your personal *__cheat sheet__*!


\newpage

# Chi-Square test

If you only have categorical variables, this is a test you can use. 

\bluebox{Question: Is there a connection between two categorical variables?}

:::: {.bluebox data-latex=""}
::: {.center2 data-latex=""}
Question: Is there a connection between two categorical variables?
:::
:::: 

*Example: Is there a connection between the variables "Southern State" & "High Youth Unemployment" (both yes/no)? For example: Is high youth unemployment more likely in Southern States?*

### __--- Hypotheses ---__

\greenwhitebox{H0: The variables are independent from each other \newline  
H1: The variables influence each other}

:::: {.greenbox data-latex=""}
::: {.center data-latex=""}
H0: The variables are independent from each other
:::
::: {.center data-latex=""}
H1: The variables influence each other
:::
::::  

\yellowbox{p > 0.05 → variables influence each other \newline
p $\leq$ 0.05 → variables are independent from each other}

:::: {.yellowbox data-latex=""}
::: {.center data-latex=""}
p > 0.05 → variables influence each other
:::
::: {.center data-latex=""}
p $\leq$ 0.05 → variables are independent from each other
:::
::::  

### __--- Descriptives ---__

*Contingency table* (Southern State & High Youth Unemployment)

```{r, eval = T}
crime$Southern <- ifelse(crime$Southern == "0", "N", "S") # recode for better understanding
ct <- table(crime$HighYouthUnemploy,crime$Southern)
ct
```

There are 14 states with high youth unemployment for Northern States and 1 for Southern States and 17 where there isn't high youth unemployment for Northern and 15 for Southern States.

### __--- Visualization ---__

A barplot of the contingency table gives us an intuition about how the data is distributed.

```{r, eval = T, fig.width= 3, fig.height= 3}
par(mar = c(2,2,1,1)) # ignore this line
barplot(ct)
```

### __--- Perform the test ---__

```{r, eval = T}
chisq.test(crime$Southern, crime$HighYouthUnemploy)
```

### __--- Answer ---__

There seems to be a connection between these variables. They are not independent of each other.

